{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL with compression\n",
    "\n",
    "In this notebook, we will show how to use lossy compressor in FL to compress the model parameters and reduce the communication cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install compressors\n",
    "\n",
    "To install the compressors, fist make sure that you have installed necessary packages for the compressors by running the following command in the `APPFL` directory.\n",
    "\n",
    "```bash\n",
    "pip install -e . # if you installed using source code\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```bash\n",
    "pip install appfl # if you installed directly using pypi\n",
    "```\n",
    "\n",
    "Then, you can easily install the lossy compressors by running the following command anywhere. It will download all the source code for compressors under `APPFL/.compressor`.\n",
    "\n",
    "```bash\n",
    "appfl-install-compressor\n",
    "```\n",
    "\n",
    "ðŸ’¡ `appfl.compressor` supports four compressors: [SZ2](https://github.com/szcompressor/SZ), [SZ3](https://github.com/szcompressor/SZ3), [ZFP](https://pypi.org/project/zfpy/), and [SZX](https://github.com/szcompressor/SZx). However, as SZx needs particular permission to access, so we have to omit its installation here. If you want to try with SZx, please contact its authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load server and client configurations\n",
    "\n",
    "Following the same steps in the [serial FL example](serial_fl.ipynb), we load and modify the configurations for server and five clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "num_clients = 5\n",
    "\n",
    "server_config_file = \"../../examples/resources/configs/mnist/server_fedavg.yaml\"\n",
    "server_config = OmegaConf.load(server_config_file)\n",
    "server_config.client_configs.train_configs.loss_fn_path = (\n",
    "    \"../../examples/resources/loss/celoss.py\"\n",
    ")\n",
    "server_config.client_configs.train_configs.metric_path = (\n",
    "    \"../../examples/resources/metric/acc.py\"\n",
    ")\n",
    "server_config.client_configs.model_configs.model_path = (\n",
    "    \"../../examples/resources/model/cnn.py\"\n",
    ")\n",
    "server_config.server_configs.num_global_epochs = 10\n",
    "server_config.server_configs.scheduler_kwargs.num_clients = num_clients\n",
    "\n",
    "client_config_file = \"../../examples/resources/configs/mnist/client_1.yaml\"\n",
    "client_config = OmegaConf.load(client_config_file)\n",
    "client_configs = [copy.deepcopy(client_config) for _ in range(num_clients)]\n",
    "for i in range(num_clients):\n",
    "    client_configs[i].train_configs.logging_id = f\"Client_{i+1}\"\n",
    "    client_configs[\n",
    "        i\n",
    "    ].data_configs.dataset_path = \"../../examples/resources/dataset/mnist_dataset.py\"\n",
    "    client_configs[i].data_configs.dataset_kwargs.num_clients = num_clients\n",
    "    client_configs[i].data_configs.dataset_kwargs.client_id = i\n",
    "    client_configs[i].data_configs.dataset_kwargs.visualization = (\n",
    "        True if i == 0 else False\n",
    "    )\n",
    "\n",
    "print(OmegaConf.to_yaml(server_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable compression\n",
    "\n",
    "To enable compression, we just need to set `server_config.client_configs.comm_configs.compressor_configs.enable_compression` to True.\n",
    "\n",
    "ðŸ’¡ You may notices that both `server_config.server_configs` and `server_config.client_configs` have a `comm_configs` fields. Actually, when creating the server agent, its communication configurations will be the merging of `server_config.server_configs.comm_configs` and `server_config.client_configs.comm_configs`. However, `server_config.client_configs.comm_configs` will also be shared with clients, while `server_config.server_configs.comm_configs` will not. As we want the clients to be aware of the compressor configurations, we put `compressor_configs` under `server_config.client_configs.comm_configs` to share with the clients during the FL experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agents and start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from appfl.agent import ServerAgent, ClientAgent\n",
    "\n",
    "# Create server and client agents\n",
    "server_agent = ServerAgent(server_agent_config=server_config)\n",
    "client_agents = [\n",
    "    ClientAgent(client_agent_config=client_configs[i]) for i in range(num_clients)\n",
    "]\n",
    "\n",
    "# Get additional client configurations from the server\n",
    "client_config_from_server = server_agent.get_client_configs()\n",
    "for client_agent in client_agents:\n",
    "    client_agent.load_config(client_config_from_server)\n",
    "\n",
    "# Load initial global model from the server\n",
    "init_global_model = server_agent.get_parameters(serial_run=True)\n",
    "for client_agent in client_agents:\n",
    "    client_agent.load_parameters(init_global_model)\n",
    "\n",
    "# [Optional] Set number of local data to the server\n",
    "for i in range(num_clients):\n",
    "    sample_size = client_agents[i].get_sample_size()\n",
    "    server_agent.set_sample_size(\n",
    "        client_id=client_agents[i].get_id(), sample_size=sample_size\n",
    "    )\n",
    "\n",
    "while not server_agent.training_finished():\n",
    "    new_global_models = []\n",
    "    for client_agent in client_agents:\n",
    "        # Client local training\n",
    "        client_agent.train()\n",
    "        local_model = client_agent.get_parameters()\n",
    "        # \"Send\" local model to server and get a Future object for the new global model\n",
    "        # The Future object will be resolved when the server receives local models from all clients\n",
    "        new_global_model_future = server_agent.global_update(\n",
    "            client_id=client_agent.get_id(),\n",
    "            local_model=local_model,\n",
    "            blocking=False,\n",
    "        )\n",
    "        new_global_models.append(new_global_model_future)\n",
    "    # Load the new global model from the server\n",
    "    for client_agent, new_global_model_future in zip(client_agents, new_global_models):\n",
    "        client_agent.load_parameters(new_global_model_future.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APPFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
