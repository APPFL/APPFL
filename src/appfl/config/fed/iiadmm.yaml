type: iiadmm
servername: IIADMMServer
clientname: IIADMMClient

args:
  num_local_epochs: 10  
<<<<<<< Updated upstream

  ## Training Data Batch Info
  batch_training: True
  train_data_batch_size: 8
  train_data_shuffle: False
=======
>>>>>>> Stashed changes
  
  accum_grad: True
  coeff_grad: False

  ## Optimizer for a gradient calculation (esp., to use zero.grad)
  optim: SGD
  optim_args:    
    lr: 0
    # momentum: 0.9
    # weight_decay: 1e-5

  ## Penalty
  init_penalty: 100
  ## Adaptive penalty
  ## Residual balancing (see (3.13) in "Distributed Optimization and Statistical Learning via the ADMM")
  ## penalty = penalty*(tau) if prim_res > (mu)*dual_res
  ## penalty = penalty/(tau) if dual_res > (mu)*prim_res
  residual_balancing:
    res_on: False
    res_on_every_update: False
    tau: 1.1
    mu: 10


  ## Differential Privacy 
  ##  epsilon: False  (non-private)
  ##  epsilon: 1      (stronger privacy as the value decreases)
  ##  epsilon: 0.05   
  epsilon: False

  ## Gradient Clipping
  ## clip_value: False (no-clipping)
  ## clip_value: 10    (clipping)
  ## clip_value: 1  
  clip_value: False
<<<<<<< Updated upstream
  clip_norm: 1
=======
  clip_norm: 2
>>>>>>> Stashed changes


  
 
  
  